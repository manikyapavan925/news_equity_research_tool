import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import os
import pandas as pd
from urllib.parse import urlparse
import time

# Streamlit page configuration - optimized for deployment
st.set_page_config(
    page_title="News Research Assistant", 
    page_icon="üì∞", 
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/manikyapavan925/news_equity_research_tool',
        'Report a bug': 'https://github.com/manikyapavan925/news_equity_research_tool/issues',
        'About': 'News Research Assistant - A tool for analyzing financial news articles'
    }
)

# Add custom CSS for better styling
st.markdown("""
<style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #007bff;
    }
    .footer {
        text-align: center;
        margin-top: 3rem;
        padding: 2rem;
        background: #f8f9fa;
        border-radius: 10px;
    }
</style>
""", unsafe_allow_html=True)

# Main header
st.markdown("""
<div class="main-header">
    <h1>üì∞ News Research Assistant</h1>
    <p>Analyze Financial News with AI</p>
</div>
""", unsafe_allow_html=True)

# Initialize session state
if 'articles' not in st.session_state:
    st.session_state.articles = []
if 'processing' not in st.session_state:
    st.session_state.processing = False

# Sidebar for URL input
st.sidebar.header("üìù Add News Articles")
st.sidebar.markdown("Enter up to 5 news article URLs for analysis:")

urls = []
for i in range(5):
    url = st.sidebar.text_input(f"URL {i+1}", key=f"url_{i}", help=f"Enter news article URL #{i+1}")
    if url.strip():
        urls.append(url.strip())

# Add URL validation
def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

# Filter valid URLs
valid_urls = [url for url in urls if is_valid_url(url)]
if urls and len(valid_urls) != len(urls):
    st.sidebar.warning(f"‚ö†Ô∏è {len(urls) - len(valid_urls)} invalid URL(s) detected and will be skipped.")

# Function to load LLM for AI-powered answering
@st.cache_resource
def load_simple_llm():
    """Load a lightweight LLM for question answering"""
    try:
        # Use a simple text generation approach for better reliability on Streamlit Cloud
        return "simple_qa"  # Placeholder for now, will implement actual LLM loading
    except Exception as e:
        st.error(f"Error loading LLM: {str(e)}")
        return None

# Function for AI-powered question answering
def ai_powered_answer(question, articles, mode="Detailed", use_context=True):
    """Generate AI-powered answers using article content and LLM reasoning"""
    
    # Combine all article content with better structure
    combined_content = ""
    article_titles = []
    full_articles = []
    
    for i, article in enumerate(articles):
        content = article.get('content', '')
        title = article.get('title', f'Article {i+1}')
        article_titles.append(title)
        
        if content:
            # Store full article for detailed analysis
            full_articles.append({
                'title': title,
                'content': content,
                'url': article.get('url', ''),
                'domain': article.get('domain', '')
            })
            # Combine content with better formatting
            combined_content += f"\n\n=== {title} ===\n{content[:2000]}"  # Increased limit
    
    if not combined_content.strip():
        return [{
            'type': 'ai_response',
            'title': 'AI Analysis',
            'content': "‚ö†Ô∏è No article content available for AI analysis.",
            'confidence': 0
        }]
    
    # Enhanced question analysis and information extraction
    question_analysis = analyze_question_intent(question)
    relevant_info = extract_question_specific_information(question, full_articles, question_analysis)
    
    # Generate more sophisticated response
    if relevant_info['found_specific_info']:
        ai_response = generate_detailed_response(question, relevant_info, mode, question_analysis)
        confidence = calculate_enhanced_confidence(relevant_info, question_analysis)
        
        # Calculate accuracy score with better metrics
        accuracy_data = calculate_answer_accuracy(question, ai_response, relevant_info['key_sentences'], articles)
        
        return [{
            'type': 'ai_response',
            'title': f'AI Analysis ({mode})',
            'content': ai_response,
            'confidence': confidence,
            'accuracy': accuracy_data,
            'sources': article_titles,
            'specific_findings': relevant_info.get('specific_data', [])
        }]
    else:
        # Provide helpful guidance when no specific info found
        fallback_response = generate_fallback_response(question, article_titles, full_articles)
        
        accuracy_data = {
            'overall_accuracy': 0.3,
            'content_relevance': 0.2,
            'factual_consistency': 0.3,
            'information_coverage': 0.1,
            'source_reliability': 0.5,
            'details': {
                'question_words_matched': 0,
                'total_question_words': len(set(re.findall(r'\b\w+\b', question.lower()))),
                'sources_analyzed': len(articles),
                'key_info_pieces': 0
            }
        }
        
        return [{
            'type': 'ai_response',
            'title': 'AI Analysis',
            'content': fallback_response,
            'confidence': 0.3,
            'accuracy': accuracy_data
        }]

def analyze_question_intent(question):
    """Analyze the question to understand what type of information is being requested"""
    question_lower = question.lower()
    
    intent = {
        'type': 'general',
        'entities': [],
        'data_type': None,
        'timeframe': None,
        'specificity': 'general'
    }
    
    # Identify question type
    if any(word in question_lower for word in ['what is', 'what are', 'describe', 'explain', 'tell me about']):
        intent['type'] = 'descriptive'
    elif any(word in question_lower for word in ['why', 'reason', 'cause', 'because']):
        intent['type'] = 'causal'
    elif any(word in question_lower for word in ['how', 'process', 'method', 'way']):
        intent['type'] = 'procedural'
    elif any(word in question_lower for word in ['when', 'time', 'date', 'timeline']):
        intent['type'] = 'temporal'
    elif any(word in question_lower for word in ['where', 'location', 'place']):
        intent['type'] = 'location'
    elif any(word in question_lower for word in ['who', 'person', 'company', 'organization']):
        intent['type'] = 'entity'
    elif any(word in question_lower for word in ['how much', 'how many', 'price', 'cost', 'value', 'amount']):
        intent['type'] = 'quantitative'
    
    # Identify specific entities
    entities = []
    companies = ['microsoft', 'msft', 'apple', 'aapl', 'google', 'googl', 'amazon', 'amzn', 'tesla', 'tsla', 'meta', 'fb']
    for company in companies:
        if company in question_lower:
            entities.append(company.upper() if len(company) <= 4 else company.title())
    intent['entities'] = entities
    
    # Identify data type being requested
    if any(word in question_lower for word in ['target price', 'price target', 'target', 'forecast price', 'analyst target']):
        intent['data_type'] = 'target_price'
    elif any(word in question_lower for word in ['current price', 'stock price', 'share price', 'trading price']):
        intent['data_type'] = 'current_price'
    elif any(word in question_lower for word in ['price', 'cost', 'trading', 'value', 'worth', 'quote']):
        intent['data_type'] = 'price'
    elif any(word in question_lower for word in ['earnings', 'revenue', 'profit', 'income', 'eps']):
        intent['data_type'] = 'earnings'
    elif any(word in question_lower for word in ['volume', 'shares', 'traded']):
        intent['data_type'] = 'volume'
    elif any(word in question_lower for word in ['news', 'announcement', 'report']):
        intent['data_type'] = 'news'
    elif any(word in question_lower for word in ['trend', 'pattern', 'movement']):
        intent['data_type'] = 'trend'
    
    # Identify timeframe
    if any(word in question_lower for word in ['current', 'now', 'today', 'latest', 'recent']):
        intent['timeframe'] = 'current'
    elif any(word in question_lower for word in ['yesterday', 'past', 'previous', 'last']):
        intent['timeframe'] = 'past'
    elif any(word in question_lower for word in ['future', 'will', 'forecast', 'prediction']):
        intent['timeframe'] = 'future'
    
    # Determine specificity
    specific_indicators = ['exact', 'specific', 'precise', 'current', 'latest']
    if any(indicator in question_lower for indicator in specific_indicators):
        intent['specificity'] = 'specific'
    
    return intent

def extract_question_specific_information(question, articles, question_analysis):
    """Extract information specifically relevant to the question being asked"""
    question_lower = question.lower()
    question_words = set(re.findall(r'\b\w+\b', question_lower))
    
    # Remove stop words for better matching
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'this', 'that'}
    meaningful_words = question_words - stop_words
    
    relevant_info = {
        'found_specific_info': False,
        'key_sentences': [],
        'specific_data': [],
        'context_sentences': [],
        'relevance_scores': []
    }
    
    # Search through all articles for relevant information
    for article in articles:
        content = article.get('content', '')
        title = article.get('title', '')
        
        if not content:
            continue
            
        # Split into sentences for analysis
        sentences = re.split(r'[.!?]+', content)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20:  # Skip very short sentences
                continue
                
            sentence_lower = sentence.lower()
            relevance_score = 0
            
            # Score based on question intent
            if question_analysis['type'] == 'quantitative':
                # Look for numbers, prices, percentages
                if re.search(r'\$\d+\.?\d*|\d+\.\d+%|\d+%|\d+,\d+|\d+\s*(million|billion|trillion)', sentence):
                    relevance_score += 5
                    
            if question_analysis['data_type'] == 'target_price':
                # Specifically look for target price information
                target_indicators = ['target', 'forecast', 'analyst', 'projection', 'outlook', 'call', 'recommendation']
                if any(indicator in sentence_lower for indicator in target_indicators):
                    relevance_score += 5
                    
            if question_analysis['data_type'] in ['price', 'current_price']:
                # Specifically look for price information
                price_indicators = ['price', 'trading', 'cost', 'value', 'worth', '$', 'dollar', 'cent']
                if any(indicator in sentence_lower for indicator in price_indicators):
                    relevance_score += 4
                    
            if question_analysis['data_type'] == 'earnings':
                # Look for earnings-related information
                earnings_indicators = ['earnings', 'revenue', 'profit', 'income', 'eps', 'quarterly', 'annual']
                if any(indicator in sentence_lower for indicator in earnings_indicators):
                    relevance_score += 4
            
            # Score based on entity matches
            for entity in question_analysis['entities']:
                if entity.lower() in sentence_lower:
                    relevance_score += 3
                    
            # Score based on keyword matches
            for word in meaningful_words:
                if len(word) > 2 and word in sentence_lower:
                    relevance_score += 1
                    
            # Score based on timeframe
            if question_analysis['timeframe'] == 'current':
                current_indicators = ['today', 'current', 'now', 'latest', 'recent', 'this week', 'this month']
                if any(indicator in sentence_lower for indicator in current_indicators):
                    relevance_score += 2
            
            # Extract specific data if found
            if relevance_score > 0:
                # Clean the sentence first
                clean_sentence = re.sub(r'[^\w\s$.,%-]', ' ', sentence)  # Remove special characters
                clean_sentence = re.sub(r'\s+', ' ', clean_sentence).strip()  # Normalize whitespace
                
                # Extract prices with better patterns
                price_patterns = [
                    r'\$(\d{1,4}(?:,\d{3})*(?:\.\d{2})?)',  # $123.45, $1,234.56
                    r'(\d{1,4}(?:,\d{3})*(?:\.\d{2})?)\s*dollars?',  # 123.45 dollars
                    r'target\s+price\s*:?\s*\$?(\d{1,4}(?:,\d{3})*(?:\.\d{2})?)',  # target price: $123
                    r'price\s+target\s*:?\s*\$?(\d{1,4}(?:,\d{3})*(?:\.\d{2})?)',  # price target: $123
                    r'(\d{3,4})\s*calls?\s+on',  # 395 calls on (for options)
                ]
                
                for pattern in price_patterns:
                    matches = re.findall(pattern, clean_sentence, re.IGNORECASE)
                    for match in matches:
                        # Clean up the match
                        clean_price = match.replace(',', '') if isinstance(match, str) else str(match)
                        if clean_price.replace('.', '').isdigit() and float(clean_price) > 10:  # Reasonable price range
                            relevant_info['specific_data'].append(f"Target Price: ${clean_price}")
                
                # Extract regular prices
                regular_price_matches = re.findall(r'\$(\d{1,4}(?:,\d{3})*(?:\.\d{2})?)', clean_sentence)
                for price in regular_price_matches:
                    clean_price = price.replace(',', '')
                    if float(clean_price) > 10:  # Filter out tiny amounts
                        relevant_info['specific_data'].append(f"Price: ${clean_price}")
                    
                # Extract percentages
                percent_matches = re.findall(r'(\d+\.?\d*)%', clean_sentence)
                for percent in percent_matches:
                    relevant_info['specific_data'].append(f"Percentage: {percent}%")
                    
                # Extract dates
                date_matches = re.findall(r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},?\s+\d{4}', clean_sentence, re.IGNORECASE)
                for date in date_matches:
                    relevant_info['specific_data'].append(f"Date: {date}")
                
                # Store clean sentence instead of raw
                if relevance_score >= 2:
                    relevant_info['key_sentences'].append(clean_sentence)
                elif relevance_score > 0:
                    relevant_info['context_sentences'].append(clean_sentence)
            
            # Add to relevant sentences if score is high enough
            if relevance_score >= 2:
                relevant_info['relevance_scores'].append(relevance_score)
                relevant_info['found_specific_info'] = True
    
    # Sort by relevance score
    if relevant_info['key_sentences']:
        combined = list(zip(relevant_info['key_sentences'], relevant_info['relevance_scores']))
        combined.sort(key=lambda x: x[1], reverse=True)
        relevant_info['key_sentences'] = [item[0] for item in combined[:7]]  # Top 7 most relevant
    
    return relevant_info

def generate_detailed_response(question, relevant_info, mode, question_analysis):
    """Generate a detailed, question-specific response"""
    
    response = ""
    question_lower = question.lower()
    
    # Check for target price questions specifically
    is_target_price_question = any(word in question_lower for word in ['target price', 'price target', 'target', 'forecast price'])
    is_price_question = any(word in question_lower for word in ['price', 'cost', 'value', 'worth'])
    
    # Start with direct answer if we have specific data
    if relevant_info['specific_data']:
        # Filter and clean specific data for direct answers
        clean_data = []
        target_prices = []
        regular_prices = []
        
        for data_point in relevant_info['specific_data']:
            if 'Target Price:' in data_point:
                target_prices.append(data_point.replace('Target Price: ', ''))
            elif 'Price:' in data_point:
                regular_prices.append(data_point.replace('Price: ', ''))
            else:
                clean_data.append(data_point)
        
        # Provide direct answer based on question type
        if is_target_price_question and target_prices:
            response += f"**üéØ Target Price Answer:**\n"
            # Remove duplicates and show unique target prices
            unique_targets = list(set(target_prices))
            for target in unique_targets[:3]:  # Show top 3 unique targets
                response += f"‚Ä¢ **{target}** (Microsoft target price)\n"
            response += "\n"
            
        elif is_price_question and (target_prices or regular_prices):
            response += f"**üí∞ Price Information:**\n"
            # Show target prices first if available
            if target_prices:
                unique_targets = list(set(target_prices))
                for target in unique_targets[:2]:
                    response += f"‚Ä¢ **Target: {target}**\n"
            if regular_prices:
                unique_prices = list(set(regular_prices))
                for price in unique_prices[:2]:
                    response += f"‚Ä¢ **Current: {price}**\n"
            response += "\n"
            
        elif clean_data:
            response += f"**ÔøΩ Key Data Found:**\n"
            # Remove duplicates and show unique data points
            unique_data = list(set(clean_data))
            for data_point in unique_data[:3]:
                response += f"‚Ä¢ {data_point}\n"
            response += "\n"
    
    # Add context only if in detailed mode and we found specific answers
    if mode == "Detailed" and relevant_info['specific_data']:
        if relevant_info['key_sentences']:
            response += f"**üìù Supporting Details:**\n"
            # Show only the most relevant sentence for context
            best_sentence = relevant_info['key_sentences'][0]
            # Clean up the sentence for display
            clean_context = re.sub(r'\s+', ' ', best_sentence).strip()
            if len(clean_context) > 200:
                clean_context = clean_context[:200] + "..."
            response += f"‚Ä¢ {clean_context}\n\n"
    
    # For concise mode, just show the direct answer
    elif mode == "Concise":
        if not relevant_info['specific_data']:
            # Fallback to first relevant sentence if no specific data
            if relevant_info['key_sentences']:
                response += f"**Answer:** {relevant_info['key_sentences'][0][:150]}..."
                
    # For analytical mode, add market context
    elif mode == "Analytical":
        if relevant_info['specific_data']:
            response += f"**üìà Market Analysis:**\n"
            response += f"‚Ä¢ The target price information suggests analyst confidence in Microsoft's future performance\n"
            response += f"‚Ä¢ Options activity at these levels indicates institutional interest\n"
            
            if question_analysis['entities']:
                response += f"‚Ä¢ Focus on {', '.join(question_analysis['entities'])} shows targeted investment strategy\n"
    
    # If no specific data found, provide a helpful message
    if not relevant_info['specific_data'] and not relevant_info['key_sentences']:
        response = f"**‚ùå No specific {question_analysis.get('data_type', 'information')} found**\n\n"
        response += f"I searched for information related to '{question}' but couldn't find specific data points. "
        response += f"The articles may discuss related topics but don't contain the exact information you're looking for.\n\n"
        response += f"**üí° Try asking:**\n"
        response += f"‚Ä¢ \"What is mentioned about Microsoft?\"\n"
        response += f"‚Ä¢ \"What are the main themes in the articles?\"\n"
        response += f"‚Ä¢ \"Summarize the Microsoft-related content\"\n"
    
    return response

def generate_fallback_response(question, article_titles, articles):
    """Generate a helpful fallback response when specific information isn't found"""
    
    question_lower = question.lower()
    
    # Analyze what topics are actually available in the articles
    available_topics = []
    companies_mentioned = set()
    
    for article in articles:
        content = article.get('content', '').lower()
        title = article.get('title', '').lower()
        
        # Extract company mentions
        common_companies = ['microsoft', 'apple', 'google', 'amazon', 'tesla', 'meta', 'nvidia', 'intel']
        for company in common_companies:
            if company in content or company in title:
                companies_mentioned.add(company.title())
        
        # Extract topic areas
        if any(word in content for word in ['earnings', 'revenue', 'profit']):
            available_topics.append('Earnings & Financial Performance')
        if any(word in content for word in ['stock', 'trading', 'price', 'market']):
            available_topics.append('Stock Market Activity')
        if any(word in content for word in ['merger', 'acquisition', 'buyout']):
            available_topics.append('M&A Activity')
        if any(word in content for word in ['regulation', 'policy', 'government']):
            available_topics.append('Regulatory News')
    
    response = f"**ü§ñ AI Analysis Results:**\n\n"
    response += f"I searched through the available articles for information related to '{question}', but couldn't find specific data to directly answer your question.\n\n"
    
    response += f"**üì∞ What I found in the articles:**\n"
    if companies_mentioned:
        response += f"‚Ä¢ **Companies mentioned:** {', '.join(sorted(companies_mentioned))}\n"
    if available_topics:
        response += f"‚Ä¢ **Topics covered:** {', '.join(set(available_topics))}\n"
    
    response += f"\n**üí° Suggested questions you could ask:**\n"
    
    if companies_mentioned:
        company = list(companies_mentioned)[0]
        response += f"‚Ä¢ \"What news is available about {company}?\"\n"
        response += f"‚Ä¢ \"What are the latest developments for {company}?\"\n"
    
    if 'Stock Market Activity' in available_topics:
        response += f"‚Ä¢ \"What are the main stock market trends?\"\n"
        response += f"‚Ä¢ \"What companies had significant stock movements?\"\n"
    
    if 'Earnings & Financial Performance' in available_topics:
        response += f"‚Ä¢ \"What earnings reports are discussed?\"\n"
        response += f"‚Ä¢ \"What are the financial highlights?\"\n"
    
    response += f"‚Ä¢ \"Summarize the main points from all articles\"\n"
    response += f"‚Ä¢ \"What are the key themes in these articles?\"\n"
    
    response += f"\n**üîç Tips for better results:**\n"
    response += f"‚Ä¢ Use keywords that appear in the article titles or content\n"
    response += f"‚Ä¢ Ask about general trends rather than specific data points\n"
    response += f"‚Ä¢ Try rephrasing your question with different terms\n"
    
    return response

def calculate_enhanced_confidence(relevant_info, question_analysis):
    """Calculate confidence based on the quality and relevance of found information"""
    
    confidence = 0.0
    
    # Base confidence on amount of relevant information found
    if relevant_info['key_sentences']:
        confidence += min(0.4, len(relevant_info['key_sentences']) * 0.1)
    
    # Boost confidence for specific data points found
    if relevant_info['specific_data']:
        confidence += min(0.3, len(relevant_info['specific_data']) * 0.1)
    
    # Boost confidence for matching question intent
    if question_analysis['type'] in ['quantitative', 'descriptive'] and relevant_info['specific_data']:
        confidence += 0.2
    
    # Boost confidence for entity matches
    if question_analysis['entities'] and relevant_info['key_sentences']:
        confidence += 0.1
    
    # Boost confidence for high relevance scores
    if relevant_info['relevance_scores']:
        avg_relevance = sum(relevant_info['relevance_scores']) / len(relevant_info['relevance_scores'])
        if avg_relevance > 4:
            confidence += 0.2
        elif avg_relevance > 2:
            confidence += 0.1
    
    return min(0.95, confidence)

def extract_key_information(content, question):
    """Extract relevant information from content based on question"""
    content_lower = content.lower()
    question_lower = question.lower()
    
    # Enhanced financial keywords mapping
    financial_keywords = {
        'stock': ['stock', 'share', 'equity', 'shares', 'ticker'],
        'price': ['price', 'cost', 'value', 'worth', 'trading', 'quote', 'priced'],
        'earnings': ['earnings', 'profit', 'revenue', 'income', 'eps'],
        'company': ['company', 'corporation', 'firm', 'business', 'corp'],
        'market': ['market', 'trading', 'exchange', 'nasdaq', 'nyse', 'dow'],
        'merger': ['merger', 'acquisition', 'buyout', 'takeover'],
        'growth': ['growth', 'increase', 'rise', 'gain', 'up', 'higher'],
        'decline': ['decline', 'decrease', 'fall', 'drop', 'loss', 'down', 'lower'],
        'microsoft': ['microsoft', 'msft', 'redmond', 'satya nadella'],
        'current': ['current', 'today', 'now', 'present', 'latest', 'recent']
    }
    
    # Find relevant sentences based on question keywords
    question_words = set(re.findall(r'\b\w+\b', question_lower))
    sentences = re.split(r'[.!?]+', content)
    relevant_info = []
    
    # Special handling for price questions
    is_price_question = any(word in question_lower for word in ['price', 'cost', 'trading', 'worth', 'value'])
    is_microsoft_question = any(word in question_lower for word in ['microsoft', 'msft'])
    
    for sentence in sentences:
        if len(sentence.strip()) > 15:
            sentence_lower = sentence.lower()
            relevance_score = 0
            
            # High priority for sentences with price information
            if is_price_question:
                price_indicators = [
                    r'\$\d+\.?\d*',  # $123.45
                    r'\d+\.\d+',     # 123.45
                    r'price', 'trading', 'worth', 'value', 'cost', 'quote'
                ]
                
                for indicator in price_indicators:
                    if re.search(indicator, sentence_lower):
                        relevance_score += 5
            
            # High priority for Microsoft-related content if asking about Microsoft
            if is_microsoft_question:
                if any(term in sentence_lower for term in ['microsoft', 'msft']):
                    relevance_score += 4
            
            # Check for direct question word matches
            for word in question_words:
                if len(word) > 2 and word in sentence_lower:  # Ignore short words
                    relevance_score += 2
            
            # Check for financial keyword matches
            for category, keywords in financial_keywords.items():
                if any(keyword in question_lower for keyword in keywords):
                    if any(keyword in sentence_lower for keyword in keywords):
                        relevance_score += 3
            
            # Bonus for sentences with numbers (likely contain specific data)
            if re.search(r'\d+', sentence):
                relevance_score += 1
            
            # Bonus for recent/current information
            if any(word in sentence_lower for word in ['today', 'current', 'now', 'latest', 'recent']):
                relevance_score += 2
            
            if relevance_score > 1:
                relevant_info.append((sentence.strip(), relevance_score))
    
    # Sort by relevance and return top sentences
    relevant_info.sort(key=lambda x: x[1], reverse=True)
    
    # If no highly relevant info found for price questions, be more lenient
    if is_price_question and len(relevant_info) < 2:
        for sentence in sentences:
            if len(sentence.strip()) > 15:
                sentence_lower = sentence.lower()
                if any(word in sentence_lower for word in ['stock', 'share', 'market', 'trading']):
                    if (sentence.strip(), 1) not in relevant_info:
                        relevant_info.append((sentence.strip(), 1))
    
    return [info[0] for info in relevant_info[:7]]  # Return more results for better context

def generate_contextual_response(question, key_info, mode, sources):
    """Generate a contextual response based on extracted information"""
    
    if not key_info:
        return "I couldn't find specific information related to your question in the available articles."
    
    question_lower = question.lower()
    
    # Check for specific factual questions that need direct answers
    specific_answer = find_specific_answer(question_lower, key_info)
    
    # Analyze question type
    if any(word in question_lower for word in ['what', 'about', 'describe', 'explain']):
        response_type = "descriptive"
    elif any(word in question_lower for word in ['why', 'reason', 'cause']):
        response_type = "causal"
    elif any(word in question_lower for word in ['how', 'process', 'method']):
        response_type = "procedural"
    elif any(word in question_lower for word in ['when', 'time', 'date']):
        response_type = "temporal"
    else:
        response_type = "general"
    
    # Build response based on mode and type
    if mode == "Detailed":
        response = ""
        
        # If we found a specific answer, lead with it
        if specific_answer:
            response = f"**Direct Answer:** {specific_answer}\n\n"
            response += f"**Additional Context from Analysis:**\n\n"
        else:
            response = f"Based on my analysis of the articles, here's a comprehensive answer to your question:\n\n"
        
        for i, info in enumerate(key_info[:3], 1):
            response += f"{i}. {info}\n\n"
        
        if len(key_info) > 3:
            response += f"Additional relevant information:\n"
            for info in key_info[3:]:
                response += f"‚Ä¢ {info}\n"
        
        response += f"\n**Sources analyzed:** {', '.join(sources)}"
    
    elif mode == "Concise":
        if specific_answer:
            response = f"**Answer:** {specific_answer}"
            if len(key_info) > 0:
                response += f"\n\n**Context:** {key_info[0][:200]}..."
        else:
            response = f"**Key Answer:** {key_info[0]}"
            if len(key_info) > 1:
                response += f"\n\n**Additional context:** {key_info[1]}"
    
    else:  # Analytical
        response = f"**Analysis for '{question}':**\n\n"
        
        if specific_answer:
            response += f"**Direct Answer:** {specific_answer}\n\n"
        
        response += f"**Key findings:** {key_info[0]}\n\n"
        
        if len(key_info) > 1:
            response += f"**Supporting evidence:**\n"
            for info in key_info[1:3]:
                response += f"‚Ä¢ {info}\n"
        
        response += f"\n**Implications:** This information suggests important developments that may impact market sentiment and business operations."
    
    return response

def find_specific_answer(question_lower, key_info):
    """Find specific factual answers from the extracted information"""
    
    # Join all key info for comprehensive search
    all_info = " ".join(key_info).lower()
    
    # Price-related questions
    if any(word in question_lower for word in ['price', 'cost', 'trading', 'worth', 'value']):
        # Look for price patterns like $123.45, $123, 123.45, etc.
        price_patterns = [
            r'\$\d+\.?\d*',  # $123.45 or $123
            r'\d+\.\d+\s*(?:dollars?|usd|\$)',  # 123.45 dollars
            r'(?:price|trading|worth|value|cost)(?:\s+(?:is|at|of))?\s*\$?\d+\.?\d*',  # price is $123
            r'\$?\d+\.?\d*\s*(?:per share|each|dollar)',  # $123 per share
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, all_info)
            if matches:
                return f"The current price mentioned is {matches[0]}"
    
    # Date/time questions
    if any(word in question_lower for word in ['when', 'date', 'time']):
        date_patterns = [
            r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{1,2},?\s+\d{4}',
            r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',
            r'\b\d{4}-\d{2}-\d{2}\b'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, all_info, re.IGNORECASE)
            if matches:
                return f"The date mentioned is {matches[0]}"
    
    # Percentage questions
    if any(word in question_lower for word in ['percent', 'percentage', '%', 'rate']):
        percentage_patterns = [
            r'\d+\.?\d*\s*%',
            r'\d+\.?\d*\s*percent'
        ]
        
        for pattern in percentage_patterns:
            matches = re.findall(pattern, all_info)
            if matches:
                return f"The percentage mentioned is {matches[0]}"
    
    # Number/quantity questions
    if any(word in question_lower for word in ['how many', 'number', 'count', 'quantity']):
        number_patterns = [
            r'\b\d+(?:,\d{3})*(?:\.\d+)?\b'
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, all_info)
            if matches and len(matches) > 0:
                return f"The number mentioned is {matches[0]}"
    
    # Company/person name questions
    if any(word in question_lower for word in ['who', 'company', 'ceo', 'president']):
        # Look for capitalized names/companies
        name_patterns = [
            r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b',  # First Last
            r'\b[A-Z]{2,}\b'  # Acronyms like MSFT, AAPL
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, " ".join(key_info))
            if matches:
                return f"The entity mentioned is {matches[0]}"
    
    return None

def calculate_response_confidence(key_info, question):
    """Calculate confidence score for the AI response"""
    if not key_info:
        return 0.0
    
    # Base confidence on amount and quality of information found
    base_confidence = min(0.9, len(key_info) * 0.2)
    
    # Adjust based on information quality
    avg_length = sum(len(info) for info in key_info) / len(key_info)
    if avg_length > 50:
        base_confidence += 0.1
    
    return min(0.95, base_confidence)

def calculate_answer_accuracy(question, response_content, key_info, articles):
    """Calculate accuracy score based on multiple factors"""
    
    # Initialize accuracy components
    content_relevance = 0.0
    factual_consistency = 0.0
    information_coverage = 0.0
    source_reliability = 0.0
    
    question_lower = question.lower()
    response_lower = response_content.lower()
    
    # 1. Content Relevance (0-1): How well the response addresses the question
    question_words = set(re.findall(r'\b\w+\b', question_lower))
    response_words = set(re.findall(r'\b\w+\b', response_lower))
    
    # Remove stop words for better analysis
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    meaningful_question_words = question_words - stop_words
    meaningful_response_words = response_words - stop_words
    
    if meaningful_question_words:
        overlap = len(meaningful_question_words.intersection(meaningful_response_words))
        content_relevance = min(1.0, overlap / len(meaningful_question_words))
    
    # 2. Factual Consistency (0-1): How well the response matches source content
    if key_info:
        total_source_content = ' '.join(key_info).lower()
        source_words = set(re.findall(r'\b\w+\b', total_source_content))
        
        # Check how many response facts can be traced back to sources
        response_claims = extract_factual_claims(response_content)
        verified_claims = 0
        
        for claim in response_claims:
            claim_words = set(re.findall(r'\b\w+\b', claim.lower()))
            if claim_words.intersection(source_words):
                verified_claims += 1
        
        if response_claims:
            factual_consistency = verified_claims / len(response_claims)
        else:
            factual_consistency = 0.8  # Default for responses without specific claims
    
    # 3. Information Coverage (0-1): How comprehensive the response is
    if key_info:
        # Check if response covers multiple pieces of key information
        coverage_score = 0
        for info in key_info:
            info_words = set(re.findall(r'\b\w+\b', info.lower()))
            if info_words.intersection(meaningful_response_words):
                coverage_score += 1
        
        information_coverage = min(1.0, coverage_score / len(key_info))
    
    # 4. Source Reliability (0-1): Based on article quality and content length
    total_content_length = sum(len(article.get('content', '')) for article in articles)
    article_count = len(articles)
    
    if total_content_length > 2000:
        source_reliability = 0.9
    elif total_content_length > 1000:
        source_reliability = 0.7
    elif total_content_length > 500:
        source_reliability = 0.5
    else:
        source_reliability = 0.3
    
    # Bonus for multiple sources
    if article_count > 1:
        source_reliability = min(1.0, source_reliability + 0.1)
    
    # Calculate weighted accuracy score
    weights = {
        'content_relevance': 0.35,
        'factual_consistency': 0.30,
        'information_coverage': 0.25,
        'source_reliability': 0.10
    }
    
    accuracy_score = (
        content_relevance * weights['content_relevance'] +
        factual_consistency * weights['factual_consistency'] +
        information_coverage * weights['information_coverage'] +
        source_reliability * weights['source_reliability']
    )
    
    # Return detailed breakdown
    return {
        'overall_accuracy': min(0.98, accuracy_score),
        'content_relevance': content_relevance,
        'factual_consistency': factual_consistency,
        'information_coverage': information_coverage,
        'source_reliability': source_reliability,
        'details': {
            'question_words_matched': len(meaningful_question_words.intersection(meaningful_response_words)),
            'total_question_words': len(meaningful_question_words),
            'sources_analyzed': len(articles),
            'key_info_pieces': len(key_info) if key_info else 0
        }
    }

def extract_factual_claims(text):
    """Extract potential factual claims from response text"""
    # Split into sentences and identify factual statements
    sentences = re.split(r'[.!?]+', text)
    factual_claims = []
    
    # Look for sentences that make specific claims
    factual_indicators = [
        'said', 'reported', 'announced', 'revealed', 'showed', 'indicated',
        'increased', 'decreased', 'rose', 'fell', 'gained', 'lost',
        'will', 'plans to', 'expects', 'projected', 'estimated'
    ]
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 20:  # Meaningful length
            sentence_lower = sentence.lower()
            if any(indicator in sentence_lower for indicator in factual_indicators):
                factual_claims.append(sentence)
            # Also include sentences with numbers (likely factual)
            elif re.search(r'\d+', sentence):
                factual_claims.append(sentence)
    
    return factual_claims[:5]  # Limit to avoid overwhelming analysis

def format_accuracy_display(accuracy_data):
    """Format accuracy information for display"""
    overall = accuracy_data['overall_accuracy']
    
    # Determine accuracy level and color
    if overall >= 0.8:
        level = "High"
        color = "success"
        icon = "‚úÖ"
    elif overall >= 0.6:
        level = "Medium"
        color = "warning"
        icon = "‚ö†Ô∏è"
    else:
        level = "Low"
        color = "error"
        icon = "‚ùå"
    
    return {
        'level': level,
        'color': color,
        'icon': icon,
        'percentage': f"{overall:.1%}"
    }

# Function to generate article summary
@st.cache_data(ttl=3600)
def generate_article_summary(article, length="Medium"):
    """Generate a summary of the article content"""
    content = article.get('content', '')
    title = article.get('title', 'No Title')
    
    if not content or len(content.strip()) < 100:
        return "‚ö†Ô∏è Article content is too short to generate a meaningful summary."
    
    # Split content into sentences
    sentences = re.split(r'[.!?]+', content)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    
    if not sentences:
        return "‚ö†Ô∏è Unable to extract meaningful sentences from the article."
    
    # Determine summary length
    if length == "Short":
        target_sentences = min(3, len(sentences))
    elif length == "Medium":
        target_sentences = min(5, len(sentences))
    else:  # Detailed
        target_sentences = min(8, len(sentences))
    
    # Simple extractive summarization
    # Score sentences based on:
    # 1. Position (first and last sentences often important)
    # 2. Length (medium length sentences preferred)
    # 3. Keyword frequency
    
    # Extract important keywords from title
    title_words = set(re.findall(r'\b\w+\b', title.lower()))
    financial_keywords = {
        'stock', 'market', 'price', 'earnings', 'revenue', 'profit', 'loss', 
        'merger', 'acquisition', 'ipo', 'dividend', 'investment', 'trading',
        'financial', 'economic', 'business', 'company', 'corporation', 'shares',
        'nasdaq', 'nyse', 'dow', 'sp500', 'index', 'fund', 'bond', 'crypto'
    }
    
    scored_sentences = []
    for i, sentence in enumerate(sentences):
        score = 0
        sentence_lower = sentence.lower()
        sentence_words = set(re.findall(r'\b\w+\b', sentence_lower))
        
        # Position scoring
        if i == 0:  # First sentence
            score += 3
        elif i == len(sentences) - 1:  # Last sentence
            score += 2
        elif i < len(sentences) * 0.3:  # Early sentences
            score += 1
        
        # Length scoring (prefer medium-length sentences)
        word_count = len(sentence.split())
        if 10 <= word_count <= 30:
            score += 2
        elif 5 <= word_count <= 50:
            score += 1
        
        # Keyword scoring
        title_matches = len(title_words.intersection(sentence_words))
        financial_matches = len(financial_keywords.intersection(sentence_words))
        score += title_matches * 2 + financial_matches
        
        # Avoid very short or very long sentences
        if word_count < 5 or word_count > 60:
            score -= 2
            
        scored_sentences.append((sentence, score, i))
    
    # Sort by score and select top sentences
    scored_sentences.sort(key=lambda x: x[1], reverse=True)
    selected_sentences = scored_sentences[:target_sentences]
    
    # Sort selected sentences by original order
    selected_sentences.sort(key=lambda x: x[2])
    
    # Create summary
    summary_sentences = [s[0] for s in selected_sentences]
    summary = '. '.join(summary_sentences)
    
    # Add some basic cleanup
    summary = re.sub(r'\s+', ' ', summary).strip()
    
    if not summary:
        summary = sentences[0] if sentences else "Unable to generate summary."
    
    return summary

# Enhanced article extraction function
@st.cache_data(ttl=3600)  # Cache for 1 hour
def extract_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form']):
            element.decompose()
        
        # Try multiple selectors to find main content
        content = ""
        selectors = [
            'article',
            '.article-content', 
            '.post-content', 
            '.entry-content',
            '.content-body',
            '.story-body',
            'main',
            '.content',
            '[role="main"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text()
                break
        
        if not content:
            # Fallback to body content
            body = soup.find('body')
            if body:
                content = body.get_text()
            else:
                content = soup.get_text()
        
        # Clean and process content
        content = re.sub(r'\s+', ' ', content).strip()
        
        # Extract title
        title = ""
        title_element = soup.find('title')
        if title_element:
            title = title_element.get_text().strip()
        
        # Extract meta description
        description = ""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '').strip()
        
        return {
            'content': content[:5000],  # Limit content length
            'title': title[:200],
            'description': description[:300],
            'word_count': len(content.split()),
            'domain': urlparse(url).netloc
        }
        
    except requests.exceptions.Timeout:
        return {'error': 'Request timeout - article took too long to load'}
    except requests.exceptions.RequestException as e:
        return {'error': f'Network error: {str(e)}'}
    except Exception as e:
        return {'error': f'Error extracting content: {str(e)}'}

# Enhanced sentiment analysis
def analyze_sentiment(text):
    text_lower = text.lower()
    
    positive_words = [
        'good', 'great', 'excellent', 'positive', 'growth', 'profit', 'success', 
        'up', 'rise', 'increase', 'gain', 'strong', 'bullish', 'optimistic',
        'beat', 'outperform', 'surge', 'boost', 'rally', 'upward'
    ]
    
    negative_words = [
        'bad', 'poor', 'negative', 'loss', 'decline', 'down', 'fall', 'decrease', 
        'crisis', 'problem', 'weak', 'bearish', 'pessimistic', 'miss', 'underperform',
        'plunge', 'crash', 'drop', 'concern', 'risk', 'warning'
    ]
    
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)
    
    total_words = len(text_lower.split())
    pos_ratio = pos_count / max(total_words, 1)
    neg_ratio = neg_count / max(total_words, 1)
    
    sentiment_score = pos_count - neg_count
    confidence = abs(sentiment_score) / max(pos_count + neg_count, 1)
    
    return {
        'score': sentiment_score,
        'positive_count': pos_count,
        'negative_count': neg_count,
        'confidence': confidence,
        'positive_ratio': pos_ratio,
        'negative_ratio': neg_ratio
    }

# Process URLs button with enhanced features
col1, col2 = st.sidebar.columns(2)
with col1:
    process_button = st.button("üìÑ Process Articles", disabled=not valid_urls or st.session_state.processing)
with col2:
    clear_button = st.button("üóëÔ∏è Clear All", help="Clear all processed articles")

if clear_button:
    st.session_state.articles = []
    st.success("All articles cleared!")
    st.rerun()

if process_button and valid_urls:
    st.session_state.processing = True
    articles_data = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, url in enumerate(valid_urls):
        status_text.text(f"Processing article {i+1}/{len(valid_urls)}: {url[:50]}...")
        progress_bar.progress((i) / len(valid_urls))
        
        article_data = extract_article_content(url)
        
        if 'error' not in article_data:
            article_data['url'] = url
            article_data['processed_at'] = time.strftime("%Y-%m-%d %H:%M:%S")
            article_data['sentiment'] = analyze_sentiment(article_data['content'])
            articles_data.append(article_data)
        else:
            st.sidebar.error(f"Failed to process {url}: {article_data['error']}")
    
    progress_bar.progress(1.0)
    status_text.text("Processing complete!")
    
    if articles_data:
        st.session_state.articles = articles_data
        st.success(f"‚úÖ Successfully processed {len(articles_data)} out of {len(valid_urls)} articles!")
    else:
        st.error("‚ùå No articles could be processed successfully.")
    
    st.session_state.processing = False
    time.sleep(1)
    status_text.empty()
    progress_bar.empty()

# Display processing statistics
if valid_urls:
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"**üìä Queue Status:**")
    st.sidebar.markdown(f"‚Ä¢ Valid URLs: {len(valid_urls)}")
    st.sidebar.markdown(f"‚Ä¢ Processed: {len(st.session_state.articles)}")

# Main content area
if st.session_state.articles:
    # Summary dashboard
    st.header("üìä Analysis Dashboard")
    
    # Metrics
    total_articles = len(st.session_state.articles)
    total_words = sum(article.get('word_count', 0) for article in st.session_state.articles)
    avg_words = total_words // total_articles if total_articles > 0 else 0
    
    # Sentiment overview
    positive_articles = sum(1 for article in st.session_state.articles 
                          if article.get('sentiment', {}).get('score', 0) > 0)
    negative_articles = sum(1 for article in st.session_state.articles 
                          if article.get('sentiment', {}).get('score', 0) < 0)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Articles", total_articles)
    with col2:
        st.metric("Total Words", f"{total_words:,}")
    with col3:
        st.metric("Avg Words/Article", f"{avg_words:,}")
    with col4:
        st.metric("Positive Sentiment", f"{positive_articles}/{total_articles}")
    
    # Sentiment distribution chart
    if total_articles > 0:
        sentiment_data = pd.DataFrame([
            {'Sentiment': 'Positive', 'Count': positive_articles},
            {'Sentiment': 'Negative', 'Count': negative_articles},
            {'Sentiment': 'Neutral', 'Count': total_articles - positive_articles - negative_articles}
        ])
        st.bar_chart(sentiment_data.set_index('Sentiment'))

    # Individual article analysis
    st.header("ÔøΩ Article Details")
    
    for i, article in enumerate(st.session_state.articles):
        with st.expander(f"üì∞ Article {i+1}: {article.get('title', 'No Title')[:80]}..."):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.write(f"**üîó Source:** {article['url']}")
                st.write(f"**üåê Domain:** {article.get('domain', 'Unknown')}")
                st.write(f"**üìÖ Processed:** {article.get('processed_at', 'Unknown')}")
                
                if article.get('description'):
                    st.write(f"**üìù Description:** {article['description']}")
                
                st.write("**üìÑ Content Preview:**")
                st.write(article['content'][:500] + "..." if len(article['content']) > 500 else article['content'])
            
            with col2:
                # Sentiment analysis display
                sentiment = article.get('sentiment', {})
                score = sentiment.get('score', 0)
                confidence = sentiment.get('confidence', 0)
                
                if score > 0:
                    st.success(f"üòä Positive\nScore: +{score}\nConfidence: {confidence:.2%}")
                elif score < 0:
                    st.error(f"üòü Negative\nScore: {score}\nConfidence: {confidence:.2%}")
                else:
                    st.info(f"üòê Neutral\nScore: {score}")
                
                # Word count
                st.metric("Word Count", article.get('word_count', 0))
            
            # Extract potential stock tickers
            ticker_pattern = r'\b[A-Z]{2,5}\b'
            tickers = re.findall(ticker_pattern, article['content'])
            # Filter out common false positives
            exclude_words = {
                'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'HER', 'WAS', 'ONE', 'OUR',
                'OUT', 'DAY', 'GET', 'HAS', 'HIM', 'HIS', 'HOW', 'ITS', 'NEW', 'NOW', 'OLD', 'SEE', 'TWO',
                'WHO', 'BOY', 'DID', 'WHY', 'LET', 'PUT', 'SAY', 'SHE', 'TOO', 'USE', 'CEO', 'CFO', 'CTO',
                'USA', 'EUR', 'USD', 'API', 'CEO', 'IPO', 'ETF', 'SEC', 'LLC', 'INC', 'LTD', 'PLC'
            }
            potential_tickers = [t for t in set(tickers) if t not in exclude_words and len(t) <= 5][:10]
            
            if potential_tickers:
                st.write("**üìà Potential Stock Tickers:**")
                ticker_cols = st.columns(min(len(potential_tickers), 5))
                for idx, ticker in enumerate(potential_tickers[:5]):
                    with ticker_cols[idx]:
                        st.code(ticker)

# Enhanced Q&A Section
st.header("üí¨ Smart Question Answering")
if st.session_state.articles:
    # Check for quick question from URL params
    query_params = st.query_params
    default_question = query_params.get("question", "")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        question = st.text_input("‚ùì Ask a question about the loaded articles:", 
                                value=default_question,
                                placeholder="e.g., What companies were mentioned? What are the main themes?")
    with col2:
        search_type = st.selectbox("Search Type", ["Keyword", "Semantic", "AI-Powered"], help="Keyword: exact word matching, Semantic: meaning-based, AI-Powered: LLM reasoning")
    
    # Add advanced AI toggle
    if search_type == "AI-Powered":
        st.info("ü§ñ AI-Powered mode uses LLM reasoning to answer questions intelligently, even if exact keywords aren't present in the article.")
        
        col3, col4 = st.columns([2, 1])
        with col3:
            ai_mode = st.selectbox("AI Response Mode", ["Detailed", "Concise", "Analytical"], 
                                 help="Detailed: Comprehensive analysis, Concise: Brief summary, Analytical: Deep insights")
        with col4:
            use_context = st.checkbox("Include Context", value=True, help="Include article context in AI response")
    
    # Clear query params if question was loaded
    if default_question:
        st.query_params.clear()
    
    # Add automatic search trigger for quick questions
    auto_search = bool(default_question)  # Auto-search if question came from quick button
    
    # Quick question suggestions (before search processing)
    if not auto_search:  # Only show if not auto-searching
        st.markdown("**üí° Quick Questions:**")
        quick_questions = [
            "What companies were mentioned?",
            "What are the main financial trends?",
            "Are there any merger or acquisition news?",
            "What earnings reports are discussed?",
            "Are there any regulatory changes mentioned?"
        ]
        
        cols = st.columns(len(quick_questions))
        for i, q in enumerate(quick_questions):
            with cols[i]:
                if st.button(q, key=f"quick_q_{i}", help="Click to use this question"):
                    st.query_params["question"] = q
                    st.rerun()
    
    if (question and st.button("üîç Search Answer")) or auto_search:
        with st.spinner("Searching for relevant information..."):
            # Debug info
            st.info(f"üîç Searching {len(st.session_state.articles)} article(s) for: '{question}'")
            
            results = []
            question_lower = question.lower()
            question_words = set(re.findall(r'\b\w+\b', question_lower))
            
            # Remove common stop words for better matching
            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'this', 'that', 'what', 'when', 'where', 'how', 'why', 'who'}
            meaningful_words = question_words - stop_words
            
            st.write(f"üî§ Key search words: {list(meaningful_words)}")
            
            # Handle general questions like "what is this about"
            general_questions = [
                "what is this about", "summarize", "summary", "what is this article about",
                "tell me about this", "what does this say", "main points", "key information"
            ]
            
            is_general_question = any(general in question_lower for general in general_questions)
            
            if is_general_question:
                st.info("ü§ñ Detected general question - providing article summaries")
                
                for i, article in enumerate(st.session_state.articles):
                    content = article.get('content', '')
                    title = article.get('title', 'No Title')
                    
                    if content and len(content.strip()) > 50:
                        # Generate a quick summary for general questions
                        sentences = re.split(r'[.!?]+', content)
                        good_sentences = [s.strip() for s in sentences if 20 <= len(s.strip()) <= 150]
                        
                        # Take first few sentences as summary
                        summary_sentences = good_sentences[:3] if good_sentences else [content[:200] + "..."]
                        
                        results.append({
                            'article_index': i + 1,
                            'title': title,
                            'url': article['url'],
                            'sentences': summary_sentences,
                            'score': 10,  # High score for general questions
                            'domain': article.get('domain', 'Unknown')
                        })
            
            elif search_type == "AI-Powered":
                st.info("ü§ñ Using AI-Powered analysis to answer your question...")
                
                # Get AI-powered response
                ai_results = ai_powered_answer(question, st.session_state.articles, ai_mode, use_context)
                
                for ai_result in ai_results:
                    confidence = ai_result.get('confidence', 0.5)
                    accuracy_data = ai_result.get('accuracy', {})
                    accuracy_display = format_accuracy_display(accuracy_data)
                    
                    st.success(f"üß† AI Analysis Complete")
                    
                    # Display confidence and accuracy metrics
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("üéØ Confidence", f"{confidence:.1%}")
                    with col2:
                        st.metric(f"{accuracy_display['icon']} Accuracy", accuracy_display['percentage'])
                    with col3:
                        st.metric("üìä Relevance", f"{accuracy_data.get('content_relevance', 0):.1%}")
                    with col4:
                        st.metric("‚úÖ Consistency", f"{accuracy_data.get('factual_consistency', 0):.1%}")
                    
                    with st.expander(f"ü§ñ {ai_result['title']}", expanded=True):
                        st.markdown(ai_result['content'])
                        
                        # Detailed accuracy breakdown
                        st.markdown("### üìä Answer Quality Analysis")
                        
                        acc_col1, acc_col2 = st.columns(2)
                        with acc_col1:
                            st.markdown("**Accuracy Components:**")
                            st.progress(accuracy_data.get('content_relevance', 0), text=f"Content Relevance: {accuracy_data.get('content_relevance', 0):.1%}")
                            st.progress(accuracy_data.get('factual_consistency', 0), text=f"Factual Consistency: {accuracy_data.get('factual_consistency', 0):.1%}")
                            st.progress(accuracy_data.get('information_coverage', 0), text=f"Information Coverage: {accuracy_data.get('information_coverage', 0):.1%}")
                            st.progress(accuracy_data.get('source_reliability', 0), text=f"Source Reliability: {accuracy_data.get('source_reliability', 0):.1%}")
                        
                        with acc_col2:
                            details = accuracy_data.get('details', {})
                            st.markdown("**Analysis Details:**")
                            st.write(f"‚Ä¢ Question words matched: {details.get('question_words_matched', 0)}/{details.get('total_question_words', 0)}")
                            st.write(f"‚Ä¢ Sources analyzed: {details.get('sources_analyzed', 0)}")
                            st.write(f"‚Ä¢ Key information pieces: {details.get('key_info_pieces', 0)}")
                            
                            # Overall accuracy assessment
                            overall_acc = accuracy_data.get('overall_accuracy', 0)
                            if overall_acc >= 0.8:
                                st.success(f"üéØ High accuracy response ({overall_acc:.1%})")
                                st.info("This response is highly reliable and well-supported by the source content.")
                            elif overall_acc >= 0.6:
                                st.warning(f"‚ö†Ô∏è Medium accuracy response ({overall_acc:.1%})")
                                st.info("This response is moderately reliable. Some claims may need verification.")
                            else:
                                st.error(f"‚ùå Low accuracy response ({overall_acc:.1%})")
                                st.info("This response has limited reliability. Consider asking more specific questions or providing additional sources.")
                        
                        if ai_result.get('sources'):
                            st.markdown("**üìö Sources analyzed:**")
                            for source in ai_result['sources']:
                                st.markdown(f"‚Ä¢ {source}")
                        
                        # Confidence indicator (legacy display)
                        if confidence > 0.7:
                            st.success(f"High confidence response ({confidence:.1%})")
                        elif confidence > 0.4:
                            st.warning(f"Medium confidence response ({confidence:.1%})")
                        else:
                            st.info(f"Low confidence response ({confidence:.1%}) - Consider asking more specific questions")
                
                # Skip regular search processing for AI mode - end function here
                st.markdown("---")
                
            else:
                # Normal keyword search
                for i, article in enumerate(st.session_state.articles):
                    content = article.get('content', '')
                    title = article.get('title', '')
                    
                    # Show content length for debugging
                    st.write(f"üì∞ Article {i+1}: {len(content)} characters, Title: {title[:50]}...")
                    
                    if not content or len(content.strip()) < 50:
                        st.warning(f"‚ö†Ô∏è Article {i+1} has very little content ({len(content)} chars)")
                        continue
                    
                    content_lower = content.lower()
                    title_lower = title.lower()
                    
                    # Enhanced relevance scoring
                    if search_type == "Keyword":
                        # Exact word matching
                        content_words = set(re.findall(r'\b\w+\b', content_lower))
                        title_words = set(re.findall(r'\b\w+\b', title_lower))
                        
                        common_content_words = meaningful_words.intersection(content_words)
                        common_title_words = meaningful_words.intersection(title_words)
                        
                        # Title matches get higher weight
                        relevance_score = len(common_content_words) + (len(common_title_words) * 3)
                        
                        # If no exact matches, try partial matching
                        if relevance_score == 0 and meaningful_words:
                            for word in meaningful_words:
                                # Look for partial matches (contains word)
                                if any(word in content_word for content_word in content_words):
                                    relevance_score += 0.5
                                if any(word in title_word for title_word in title_words):
                                    relevance_score += 1.5
                        
                        st.write(f"  üéØ Content matches: {list(common_content_words)}")
                        st.write(f"  üìù Title matches: {list(common_title_words)}")
                        st.write(f"  üìä Score: {relevance_score}")
                        
                    else:
                        # Semantic matching
                        relevance_score = 0
                        for word in meaningful_words:
                            if word in content_lower:
                                relevance_score += content_lower.count(word)
                            if word in title_lower:
                                relevance_score += title_lower.count(word) * 2
                    
                    # Lower the threshold for showing results (include partial matches)
                    if relevance_score >= 0.5:
                        # Find relevant sentences
                        sentences = re.split(r'[.!?]+', content)
                        relevant_sentences = []
                        
                        for sentence in sentences:
                            sentence = sentence.strip()
                            if len(sentence) > 15:
                                sentence_lower = sentence.lower()
                                sentence_score = 0
                                
                                # Check for word matches
                                for word in meaningful_words:
                                    if word in sentence_lower:
                                        sentence_score += 2
                                    # Partial word matching
                                    elif any(word in sentence_word for sentence_word in sentence_lower.split()):
                                        sentence_score += 1
                                        
                                if sentence_score > 0:
                                    relevant_sentences.append((sentence, sentence_score))
                        
                        # Sort sentences by relevance
                        relevant_sentences.sort(key=lambda x: x[1], reverse=True)
                        
                        if relevant_sentences:
                            results.append({
                                'article_index': i + 1,
                                'title': title,
                                'url': article['url'],
                                'sentences': [s[0] for s in relevant_sentences[:5]],
                                'score': relevance_score,
                                'domain': article.get('domain', 'Unknown')
                            })
                        elif relevance_score > 0:
                            # If we have matches but no good sentences, show content preview
                            content_preview = content[:400] + "..." if len(content) > 400 else content
                            results.append({
                                'article_index': i + 1,
                                'title': title,
                                'url': article['url'],
                                'sentences': [f"Found relevant content: {content_preview}"],
                                'score': relevance_score,
                                'domain': article.get('domain', 'Unknown')
                            })
            
            # Sort results by relevance
            results.sort(key=lambda x: x['score'], reverse=True)
            
            if results:
                st.success(f"üéØ Found {len(results)} relevant source(s)")
                
                for idx, result in enumerate(results[:5]):  # Show top 5 results
                    with st.expander(f"üìÑ {result['title'][:60]}... (Relevance: {result['score']})"):
                        st.write(f"**üîó Source:** {result['url']}")
                        st.write(f"**üåê Domain:** {result['domain']}")
                        st.write(f"**üéØ Relevant excerpts:**")
                        for sentence in result['sentences']:
                            st.write(f"üí° {sentence}")
            else:
                st.warning("ü§î No relevant information found. Try rephrasing your question or using different keywords.")
else:
    st.info("üëÜ Please load some articles first to start asking questions!")
    
    # Sample articles for demonstration
    st.markdown("**üìö Try these sample financial news URLs:**")
    sample_urls = [
        "https://finance.yahoo.com/news/",
        "https://www.cnbc.com/world/?region=world",
        "https://www.reuters.com/business/finance/"
    ]
    for url in sample_urls:
        st.code(url)

# Article Summarization Section
if st.session_state.articles:
    st.header("üìÑ Article Summarization")
    
    # Select article to summarize
    article_options = [f"Article {i+1}: {article.get('title', 'No Title')[:60]}..." 
                      for i, article in enumerate(st.session_state.articles)]
    
    col1, col2 = st.columns([3, 1])
    with col1:
        selected_article_idx = st.selectbox(
            "Select an article to summarize:",
            range(len(st.session_state.articles)),
            format_func=lambda x: article_options[x]
        )
    with col2:
        summary_length = st.selectbox("Summary Length", ["Short", "Medium", "Detailed"])
    
    if st.button("üìù Generate Summary", key="summarize_btn"):
        selected_article = st.session_state.articles[selected_article_idx]
        
        with st.spinner("Generating summary..."):
            summary = generate_article_summary(selected_article, summary_length)
            
            st.subheader(f"üìÑ Summary: {selected_article.get('title', 'No Title')}")
            
            # Article metadata
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üìÖ Processed", selected_article.get('processed_at', 'Unknown'))
            with col2:
                sentiment = selected_article.get('sentiment', {})
                sentiment_label = "Positive" if sentiment.get('positive_ratio', 0) > sentiment.get('negative_ratio', 0) else "Negative"
                st.metric("üòä Sentiment", sentiment_label, f"{sentiment.get('positive_ratio', 0):.1%}")
            with col3:
                word_count = len(selected_article.get('content', '').split())
                st.metric("üìù Word Count", f"{word_count:,}")
            
            # Display summary
            st.markdown("### üéØ Key Summary")
            st.info(summary)
            
            # Original article link
            st.markdown(f"**üîó Original Article:** [{selected_article.get('domain', 'Source')}]({selected_article['url']})")
            
            # Download summary option
            summary_text = f"ARTICLE SUMMARY\n\nTitle: {selected_article.get('title', 'No Title')}\nSource: {selected_article['url']}\nProcessed: {selected_article.get('processed_at', 'Unknown')}\n\nSUMMARY:\n{summary}\n\nGenerated by News Research Assistant"
            
            st.download_button(
                label="üíæ Download Summary",
                data=summary_text,
                file_name=f"summary_{selected_article_idx+1}_{time.strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain"
            )

# Quick analysis and export features
st.header("üöÄ Quick Analysis Tools")

if st.session_state.articles:
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("üìà Market Sentiment", help="Analyze overall market sentiment"):
            # Calculate overall sentiment
            total_positive = sum(article.get('sentiment', {}).get('positive_count', 0) for article in st.session_state.articles)
            total_negative = sum(article.get('sentiment', {}).get('negative_count', 0) for article in st.session_state.articles)
            overall_score = total_positive - total_negative
            
            st.write("**üìä Overall Market Sentiment:**")
            if overall_score > 0:
                st.success(f"üü¢ Bullish sentiment detected (Score: +{overall_score})")
            elif overall_score < 0:
                st.error(f"üî¥ Bearish sentiment detected (Score: {overall_score})")
            else:
                st.info("üü° Neutral market sentiment")
    
    with col2:
        if st.button("üè¢ Extract Companies", help="Find mentioned companies"):
            # Extract company names and tickers
            all_text = " ".join([article['content'] for article in st.session_state.articles])
            
            # Pattern for company names (capitalized words)
            company_pattern = r'\b[A-Z][a-z]+ (?:Inc|Corp|Corporation|Company|Ltd|Limited|Group|Holdings)\b'
            companies = re.findall(company_pattern, all_text)
            
            # Pattern for stock tickers
            ticker_pattern = r'\b[A-Z]{2,5}\b'
            tickers = re.findall(ticker_pattern, all_text)
            
            st.write("**üè¢ Companies Mentioned:**")
            if companies:
                unique_companies = list(set(companies))[:10]
                for company in unique_companies:
                    st.write(f"‚Ä¢ {company}")
            
            st.write("**üìä Potential Tickers:**")
            if tickers:
                exclude_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'CEO', 'CFO', 'USA', 'EUR', 'USD'}
                unique_tickers = [t for t in set(tickers) if t not in exclude_words][:15]
                ticker_text = " | ".join(unique_tickers)
                st.code(ticker_text)
    
    with col3:
        if st.button("üìä Export Summary", help="Generate summary report"):
            # Create summary report
            report_data = {
                'Total Articles': len(st.session_state.articles),
                'Total Words': sum(article.get('word_count', 0) for article in st.session_state.articles),
                'Positive Articles': sum(1 for article in st.session_state.articles if article.get('sentiment', {}).get('score', 0) > 0),
                'Negative Articles': sum(1 for article in st.session_state.articles if article.get('sentiment', {}).get('score', 0) < 0),
                'Sources': len(set(article.get('domain', '') for article in st.session_state.articles))
            }
            
            st.write("**üìã Analysis Summary:**")
            for key, value in report_data.items():
                st.metric(key, value)
    
    with col4:
        if st.button("üîÑ Refresh Analysis", help="Re-analyze all articles"):
            for article in st.session_state.articles:
                article['sentiment'] = analyze_sentiment(article['content'])
            st.success("‚úÖ Analysis refreshed!")
            st.rerun()

else:
    st.info("üìù Load some articles first to access analysis tools!")

# Footer with enhanced information
st.markdown("---")
st.markdown("""
<div class="footer">
    <h3>üéØ News Research Assistant - Streamlit Cloud Edition</h3>
    <p><strong>Features Available:</strong></p>
    <ul style="text-align: left; display: inline-block;">
        <li>‚úÖ Real-time news article processing</li>
        <li>‚úÖ Advanced sentiment analysis</li>
        <li>‚úÖ Smart question answering</li>
        <li>‚úÖ Company and ticker extraction</li>
        <li>‚úÖ Interactive dashboards</li>
        <li>‚úÖ Multi-source aggregation</li>
    </ul>
</div>
""", unsafe_allow_html=True)

# Add some helpful tips in the sidebar
st.sidebar.markdown("---")
st.sidebar.markdown("### üí° Tips for Better Results")
st.sidebar.markdown("""
- **Use financial news URLs** for best results
- **Wait for processing** to complete before asking questions
- **Try specific questions** like "What companies reported earnings?"
- **Use both keyword and semantic search** for different insights
- **Check multiple sources** for comprehensive analysis
""")

st.sidebar.markdown("### üîß Troubleshooting")
st.sidebar.markdown("""
- **Slow e loading?** Some sites block automated requests
- **No content?** Check if URL is accessible and contains text
- **Empty results?** Try rephrasing your questions
- **Need help?** Check our GitHub repository
""")
